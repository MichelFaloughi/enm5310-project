Proposal:
Base model: AuroraSmallPretrained at 0.4Â° resolution.
Domain: a cropped region (e.g. Europe) rather than global.
Inputs: same Aurora state variables (2t, 10u, 10v, msl, t, u, v, z, q at all 4 levels).
Target: one new surface variable tp = 6-hour accumulated total precipitation.
Training: parameter-efficient fine-tuning:
freeze backbone
learn a new precipitation head (+ optionally a tiny set of decoder layers)
mixed precision, gradient checkpointing, batch size = 1.




Questions for me:
- What exactly is LoRA
answer: Low-Rank Adaptation of Large Language Models, meaning that instead of updating all the weights of the model during fine-tuning, you only update a small number of additional weights that are added to the model. This reduces the number of trainable parameters and makes fine-tuning more efficient.
- Is this different than freezing some of the parameters of the model during fine-tuning ?
answer: Yes, freezing parameters means that you do not update them at all during fine-tuning, while LoRA adds new parameters that are trained while the original parameters remain unchanged.
- How to implement LoRA with Aurora ? Is there an example anywhere ?
answer: Not that I know of, but you can use the PEFT library from HuggingFace to implement LoRA with Aurora. You would need to define a LoRA configuration and then wrap the Aurora model with it.
- Should I used the PEFT library from HuggingFace ? or the fine tuning functions from Aurora directly ? or both ?

Questions for Prof. Paris:
- (i guess we have no choice) Use AuroraSmallPretrained ? they don't recommend using it, but maybe for fine-tuning it's okay ? What did the guys from the paper use ?



Things to do that can mitigate GPU memory issues:
- Use gradient checkpointing to save memory during training
- Use mixed precision training (float16) to reduce memory usage
- (YES) Use smaller batch sizes
- (YES) Use AuroraSmallPretrained model instead of the larger AuroraPretrained model

- (YES) Use parameter-efficient fine-tuning (PEFT) methods like LoRA to reduce the number of trainable parameters (freezing some layers, etc.)
for p in model.parameters():
    p.requires_grad = False
then, I'll unfreeze the lightweight parts,

- choose a specific region or grid point ?
- (NO) choose only one of the atmospheric pressure levels for fine-tuning (e.g., 500 hPa).vertical structure matters for rain, memory savings would be minimal
- (YES) choose only one hydrological variable to predict (e.g. tp, pr, rain_rate). Choose tp (total precipitation) since it's the most general
- (PROBABLY) choose only one region (e.g., North America) instead of the whole globe ?
- (YES) don't aim for long lead times, stick to one (e.g., 6 hours ahead). Multi-step rollouts can come later if you have time
