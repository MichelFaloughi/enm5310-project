Proposal:
Base model: AuroraSmallPretrained at 0.4Â° resolution.
Domain: a cropped region (e.g. Europe) rather than global.
Inputs: same Aurora state variables (surface:2t, 10u, 10v, msl,
                                    static:lsm, slt, z
                                    atmos: t, u, v, z, q 
                                    meta: all 13 levels).
Target: one new surface variable tp = 6-hour accumulated total precipitation.
Training: parameter-efficient fine-tuning:
freeze backbone
learn a new precipitation head (+ optionally a tiny set of decoder layers)
mixed precision, gradient checkpointing, batch size = 1.




Questions for me:
- What exactly is LoRA
answer: Low-Rank Adaptation of Large Language Models, meaning that instead of updating all the weights of the model during fine-tuning, you only update a small number of additional weights that are added to the model. This reduces the number of trainable parameters and makes fine-tuning more efficient.
- Is this different than freezing some of the parameters of the model during fine-tuning ?
answer: Yes, freezing parameters means that you do not update them at all during fine-tuning, while LoRA adds new parameters that are trained while the original parameters remain unchanged.
- How to implement LoRA with Aurora ? Is there an example anywhere ?
answer: Not that I know of, but you can use the PEFT library from HuggingFace to implement LoRA with Aurora. You would need to define a LoRA configuration and then wrap the Aurora model with it.
- Should I used the PEFT library from HuggingFace ? or the fine tuning functions from Aurora directly ? or both ?

Questions for Prof. Paris:
- (i guess we have no choice) Use AuroraSmallPretrained ? they don't recommend using it, but maybe for fine-tuning it's okay ? What did the guys from the paper use ?



Things to do that can mitigate GPU memory issues:
- Use gradient checkpointing to save memory during training
- Use mixed precision training (float16) to reduce memory usage (model = Aurora(autocast=True?))
- (YES) Use smaller batch sizes
- (YES) Use AuroraSmallPretrained model instead of the larger AuroraPretrained model

- (YES) Use parameter-efficient fine-tuning (PEFT) methods like LoRA to reduce the number of trainable parameters (freezing some layers, etc.)
for p in model.parameters():
    p.requires_grad = False
then, I'll unfreeze the lightweight parts,

- choose a specific region or grid point ?
- (NO) choose only one of the atmospheric pressure levels for fine-tuning (e.g., 500 hPa).vertical structure matters for rain, memory savings would be minimal. use all the ones from the aurorasmallpretrained
- (YES) choose only one hydrological variable to predict (e.g. tp, pr, rain_rate). Choose tp (total precipitation) since it's the most general. visualize and see
- (PROBABLY) choose only one region (e.g., North America) instead of the whole globe ?
- (YES) don't aim for long lead times, stick to one (e.g., 6 hours ahead). Multi-step rollouts can come later if you have time





notes from the meeting:
- model was trained in 3 phases: pre-training, then finetuning was in two phases, the one-step was phase one, then rollouts was phase 2
- for our purposes, stick to one step, if you're fancy later see rollouts (but memory is gonna grow). rollout fine-tuning, is tricky, there are tricks they use you can read the aurora paper. (replay)
- so how they fine-tune the one step, they fine-tune one the full model , we can do that with AuroraSmallPretrained
- use data from 2016-2024 then test 2024-2025. (flexible) 
- the small model has 13 pressure levels ? use all of them
- we have to compute the normalization statistics for precipitation !!!
- tp is non negative, so how do you model this ? a probability type fo outcome ? ...
- maybe starting point would be to predict precipitation alone, then after to be able to predict all the other variables. because tp depends on other variables
- tp is a special case, see how others have made it
follow what ppl have done in the literature